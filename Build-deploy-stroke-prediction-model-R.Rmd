---
title: "Build and deploy a stroke prediction model using R"
date: "`r Sys.Date()`"
output: html_document
author: "Degbey Kayeon Dominique"
---

# About Data Analysis Report

This RMarkdown file contains the report of the data analysis done for the project on building and deploying a stroke prediction model in R. It contains analysis such as data exploration, summary statistics and building the prediction models. The final report was completed on `r date()`. 

**Data Description:**

According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.

This data set is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relevant information about the patient.


# Task One: Import data and data preprocessing

## Load data and install packages

```{r}
install.packages("tidymodels")
install.packages("tidyverse")
install.packages("corrplot")
install.packages("caret")
install.packages("smotefamily")
install.packages(skimr)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(tidymodels)
library(mice)
library(caret)
library(smotefamily)
library(corrplot)
library(rsample)
```

```{r}
# define the filename
filename <- "C:/Users/healthcare-dataset-stroke-data.csv"
# load the CSV file from the local directory
dataset <- read.csv(filename, header=TRUE, sep = ";")
dataset
```
```{r}

```



## Describe and explore the data

```{r}
print("
----------------------------descrption de donnees----------------------------------------------
")
summary(dataset)
print("
----------------------------Type de donnees----------------------------------------------
")
sapply(dataset,class)
print("
--------------------------------------------------------------------------
")
print(unique(dataset$gender))
print(unique(dataset$smoking_status))
```
```{r}
ggplot(dataset) +
geom_histogram(aes(x = avg_glucose_level))
```
```{r}
data_prep <- dataset
# Convertir la variable char en num
data_prep$bmi <- as.numeric(data_prep$bmi)
data_prep <- data_prep%>%
mutate(smoking_status = if_else(tolower(smoking_status) == "formerly smoked",
                                "smokes",
                                if_else(tolower(smoking_status) == "unknown",
                                        NA_character_, 
                                        smoking_status)))

data_prep
```


```{r}
data_prep1 <- subset(data_prep, gender != "Other")
data_prep1
```
```{r}
print(unique(data_prep1$gender))
print(unique(data_prep1$smoking_status))
```
```{r}
# Résumé des valeurs manquantes
md.pattern(data_prep1)
data_prep1
```


```{r}
data_prep1$bmi <- ifelse(is.na(data_prep1$bmi),
mean(data_prep1$bmi, na.rm = TRUE),
data_prep1$bmi)

data_prep1 <- data_prep1 %>%
mutate(bmi = round(bmi, 1))
```

```{r}
#stroke_recipe <- recipes::recipe(stroke ~. , data = data_prep1)%>%
#step_dummy(all_nominal(), -all_outcomes()) %>%
#step_normalize(all_numeric())
#step_impute_knn(all_predictors())
data_prep2<- na.omit(data_prep1)
print(data_prep2)
#stroke_recipe

```
```{r}
ggplot(data_prep2)+
geom_histogram(aes(x=bmi))
```
```{r}
data_numeric <- data_prep2[sapply(data_prep2, is.numeric)]
# Calculer la matrice de corrélation
cor_matrix <- cor(data_numeric, use = "complete.obs")
corrplot(cor_matrix, method = "color", type = "upper", tl.col = "black", tl.srt = 45)
```
```{r}
# summarize the class distribution
percentage <- prop.table(table(data_prep2$stroke)) * 100
cbind(freq=table(data_prep2$stroke), percentage=percentage)
```
```{r}
# Conversion des variables catégorielles en facteurs
data_prep2[] <- lapply(data_prep2, function(x) if(is.character(x)) as.factor(x) else x)
formula <- as.formula(paste("~ ."))
# Utiliser dummyVars pour créer un modèle d'encodage
dummy_model <- dummyVars(formula, data = data_prep2)
# Appliquer le modèle d'encodage pour transformer les données
data_encoded <- predict(dummy_model, newdata = data_prep2)
# Convertir le résultat en data frame
data_encoded <- as.data.frame(data_encoded)
# Ajouter la variable cible au data frame encodé
data_encoded$stroke <- data_prep2$stroke
data_encoded
```
```{r}
# rééquilibrer les données
data_smote <- SMOTE(data_encoded[ , -ncol(data_encoded)], data_encoded$stroke, K = 5, dup_size = 12)
str(data_smote)
```
```{r}
# Combinaison des données SMOTE générées avec les classes
data_balanced <- data_smote$data
data_balanced$class <- as.factor(data_smote$data$class)
# Vérification de la distribution des classes après rééquilibrage
table(data_balanced$class)
print(head(data_balanced))
```
```{r}
# summarize the class distribution
percentage <- prop.table(table(data_balanced$class)) * 100
cbind(freq=table(data_balanced$class), percentage=percentage)
```


# Task Two: Build prediction models
```{r}
data_split <-initial_split(data_balanced,
prop = 3/4)
data_split
```


```{r}
data_train <- training(data_split)
data_test <- testing(data_split)
data_cv <- vfold_cv(data_train)
```



```{r}
rf_model <- 
  rand_forest() %>%
  set_args(mtry = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")
```


```{r}
lr_model <- logistic_reg() %>%
  set_args(penalty = tune()) %>%
  set_engine("glm") %>%
  set_mode("classification")
```


```{r}
nb_model <-
  naive_Bayes()%>%
  set_args(smoothness = tune(), Laplace = tune())%>%
  set_engine("naivebayes")%>%
  set_mode("classification")

svm_model <-
  svm_linear() %>%
  set_args(cost= tune())%>%
  set_engine("kernlab") %>%
  set_mode("classification")

#SVM avec noyau radial
  svm_model_rbf <-
  svm_rbf()%>%
  set_args(cost = tune(), rbf_sigma = tune())%>%
  set_engine("kernlab") %>%
  set_mode("classification")

```

```{r}
install.packages(discrim)
library(discrim)
```

```{r}
rf_workflow <- workflow()%>%
add_model(rf_model)%>%
add_formula(class ~ .)
#add_recipe(stroke_recipe)
```


```{r}
lr_workflow <- workflow()%>%
add_model(lr_model)%>%
add_formula(class ~ .)
#add_recipe(stroke_recipe)
```


```{r}
nb_workflow <- workflow()%>%
add_model(nb_model)%>%
add_formula(class ~ .)
#add_recipe(stroke_recipe)
```


```{r}
svm_workflow <- workflow()%>%
add_model(svm_model)%>%
add_formula(class ~ .)
#add_recipe(stroke_recipe)
```


```{r}
svm_rbf_workflow <- workflow()%>%
add_model(svm_model_rbf)%>%
add_formula(class ~ .)
#add_recipe(stroke_recipe)
```

```{r}
params <- parameters(lr_model)
params2 <- parameters(svm_model)
params3 <-parameters(svm_model_rbf)
```
```{r}
rf_grid <- grid_random(mtry(range = c(1,10)),size = 10)
#lr_grid <- grid_random(params,size = 10)
nb_grid <- grid_random(smoothness(range = c(0, 1)),Laplace(range = c(0, 1)), size = 10)
svm_grid <- grid_random(params2, size = 10)
svm_rbf_grid <- grid_random(params3, size = 10)
```

```{r}
set.seed(123)
data_folds <-vfold_cv(data_balanced, v = 5)
print(data_folds$splits[[1]])
```


```{r}
dt_results <-dt_workflow %>%
tune_grid(resamples = data_folds, grid = dt_grid)
rf_tune_results%>% collect_metrics()
#lr_results%>% collect_metrics()
nb_results %>% collect_metrics()
svm_results %>% collect_metrics()
svm_rbf_results %>% collect_metrics()
```


# Task Three: Evaluate and select prediction models

```{r}

```



# Task Four: Deploy the prediction model

```{r}

```




# Task Five: Findings and Conclusions
































