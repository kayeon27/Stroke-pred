stroke_recipe <- recipes::recipe(stroke ~. , data = data_prep1)%>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_normalize(all_numeric()
#step_impute_knn(all_predictors())
#data_prep1$bmi <- ifelse(is.na(data_prep1$bmi),
#                                  mean(data_prep1$bmi, na.rm = TRUE),
#                                 data_prep1$bmi)
#data_prep1 <- data_prep1 %>%
# mutate(bmi = round(bmi, 1))
stroke_recipe <- recipes::recipe(stroke ~. , data = data_prep1)%>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_normalize(all_numeric())
#step_impute_knn(all_predictors())
#data_prep2<- na.omit(data_prep1)
#print(data_prep2)
stroke_recipe
# Pré-traiter les données en utilisant la recette
data_prepped <- prep(stroke_recipe, training = data_prep1)
data_processed <- bake(data_prepped, new_data = data_prep1)
# Afficher les premières lignes des données prétraitées
data_processed
data_prep1$bmi <- ifelse(is.na(data_prep1$bmi),
mean(data_prep1$bmi, na.rm = TRUE),
data_prep1$bmi)
data_prep1 <- data_prep1 %>%
mutate(bmi = round(bmi, 1))
#stroke_recipe <- recipes::recipe(stroke ~. , data = data_prep1)%>%
#step_dummy(all_nominal(), -all_outcomes()) %>%
#step_normalize(all_numeric())
#step_impute_knn(all_predictors())
data_prep2<- na.omit(data_prep1)
print(data_prep2)
#stroke_recipe
# Pré-traiter les données en utilisant la recette
#data_prepped <- prep(stroke_recipe, training = data_prep1)
#data_processed <- bake(data_prepped, new_data = data_prep1)
# Afficher les premières lignes des données prétraitées
#data_processed
ggplot(data_prep2)+
geom_histogram(aes(x=bmi))
data_numeric <- data_prep2[sapply(data_prep2, is.numeric)]
# Calculer la matrice de corrélation
cor_matrix <- cor(data_numeric, use = "complete.obs")
corrplot(cor_matrix, method = "color", type = "upper", tl.col = "black", tl.srt = 45)
# summarize the class distribution
percentage <- prop.table(table(data_prep2$stroke)) * 100
cbind(freq=table(data_prep2$stroke), percentage=percentage)
# Conversion des variables catégorielles en facteurs
data_prep2[] <- lapply(data_prep2, function(x) if(is.character(x)) as.factor(x) else x)
formula <- as.formula(paste("~ ."))
# Utiliser dummyVars pour créer un modèle d'encodage
dummy_model <- dummyVars(formula, data = data_prep2)
# Appliquer le modèle d'encodage pour transformer les données
data_encoded <- predict(dummy_model, newdata = data_prep2)
# Convertir le résultat en data frame
data_encoded <- as.data.frame(data_encoded)
# Ajouter la variable cible au data frame encodé
data_encoded$stroke <- data_prep2$stroke
# rééquilibrer les données
data_smote <- SMOTE(data_encoded[ , -ncol(data_encoded)], data_encoded$stroke, K = 5, dup_size = 10)
str(data_smote)
# Combinaison des données SMOTE générées avec les classes
data_balanced <- data_smote$data
data_balanced$class <- as.factor(data_smote$data$class)
# Vérification de la distribution des classes après rééquilibrage
table(data_balanced$class)
print(head(data_balanced))
# summarize the class distribution
percentage <- prop.table(table(data_balanced$class)) * 100
cbind(freq=table(data_balanced$class), percentage=percentage)
library(rsample)
data_split <-initial_split(data_balanced,
prop = 3/4)
data_split
data_train <- training(data_split)
data_test <- testing(data_split)
data_cv <- vfold_cv(data_train)
install.packages("tidymodels")
install.packages("tidyverse")
install.packages("corrplot")
install.packages("caret")
install.packages("smotefamily")
install.packages(skimr)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(tidymodels)
library(mice)
library(caret)
library(smotefamily)
library(corrplot)
library(rsample)
# define the filename
filename <- "C:/Users/healthcare-dataset-stroke-data.csv"
# load the CSV file from the local directory
dataset <- read.csv(filename, header=TRUE, sep = ";")
dataset
print("
----------------------------descrption de donnees----------------------------------------------
")
summary(dataset)
print("
----------------------------Type de donnees----------------------------------------------
")
sapply(dataset,class)
print("
--------------------------------------------------------------------------
")
print(unique(dataset$gender))
print(unique(dataset$smoking_status))
ggplot(dataset) +
geom_histogram(aes(x = avg_glucose_level))
data_prep <- dataset
# Convertir la variable char en num
data_prep$bmi <- as.numeric(data_prep$bmi)
data_prep
data_prep <- data_prep%>%
mutate(smoking_status = if_else(tolower(smoking_status) == "formerly smoked",
"smokes",
if_else(tolower(smoking_status) == "unknown",
NA_character_,
smoking_status)))
data_prep1 <- subset(data_prep, gender != "Other")
data_prep1
print(unique(data_prep1$gender))
print(unique(data_prep1$smoking_status))
# Résumé des valeurs manquantes
md.pattern(data_prep1)
data_prep1
data_prep1$bmi <- ifelse(is.na(data_prep1$bmi),
mean(data_prep1$bmi, na.rm = TRUE),
data_prep1$bmi)
data_prep1 <- data_prep1 %>%
mutate(bmi = round(bmi, 1))
#stroke_recipe <- recipes::recipe(stroke ~. , data = data_prep1)%>%
#step_dummy(all_nominal(), -all_outcomes()) %>%
#step_normalize(all_numeric())
#step_impute_knn(all_predictors())
data_prep2<- na.omit(data_prep1)
print(data_prep2)
#stroke_recipe
data_prep2<- na.omit(data_prep1)
print(data_prep2)
#stroke_recipe
ggplot(data_prep2)+
geom_histogram(aes(x=bmi))
data_numeric <- data_prep2[sapply(data_prep2, is.numeric)]
# Calculer la matrice de corrélation
cor_matrix <- cor(data_numeric, use = "complete.obs")
corrplot(cor_matrix, method = "color", type = "upper", tl.col = "black", tl.srt = 45)
# summarize the class distribution
percentage <- prop.table(table(data_prep2$stroke)) * 100
cbind(freq=table(data_prep2$stroke), percentage=percentage)
# Conversion des variables catégorielles en facteurs
data_prep2[] <- lapply(data_prep2, function(x) if(is.character(x)) as.factor(x) else x)
formula <- as.formula(paste("~ ."))
# Utiliser dummyVars pour créer un modèle d'encodage
dummy_model <- dummyVars(formula, data = data_prep2)
# Appliquer le modèle d'encodage pour transformer les données
data_encoded <- predict(dummy_model, newdata = data_prep2)
# Convertir le résultat en data frame
data_encoded <- as.data.frame(data_encoded)
# Ajouter la variable cible au data frame encodé
data_encoded$stroke <- data_prep2$stroke
data_encoded
# summarize the class distribution
percentage <- prop.table(table(data_encoded$stroke)) * 100
cbind(freq=table(data_encoded$stroke), percentage=percentage)
# rééquilibrer les données
data_smote <- SMOTE(data_encoded[ , -ncol(data_encoded)], data_encoded$stroke, K = 5, dup_size = 10)
str(data_smote)
# Combinaison des données SMOTE générées avec les classes
data_balanced <- data_smote$data
data_balanced$class <- as.factor(data_smote$data$class)
# Vérification de la distribution des classes après rééquilibrage
table(data_balanced$class)
print(head(data_balanced))
# summarize the class distribution
percentage <- prop.table(table(data_balanced$class)) * 100
cbind(freq=table(data_balanced$class), percentage=percentage)
data_split <-initial_split(data_balanced,
prop = 3/4)
data_split
data_train <- training(data_split)
data_test <- testing(data_split)
data_cv <- vfold_cv(data_train)
install.packages("tidymodels")
savehistory("~/stroke pred2.Rhistory")
setwd("C:/Users/Degbey Kayeon/Downloads/_stroke-prediction")
install.packages("tidymodels")
install.packages("tidyverse")
install.packages("corrplot")
install.packages("caret")
install.packages("smotefamily")
install.packages(skimr)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(tidymodels)
library(mice)
library(caret)
library(smotefamily)
library(corrplot)
library(rsample)
# define the filename
filename <- "C:/Users/healthcare-dataset-stroke-data.csv"
# load the CSV file from the local directory
dataset <- read.csv(filename, header=TRUE, sep = ";")
dataset
print("
----------------------------descrption de donnees----------------------------------------------
")
summary(dataset)
print("
----------------------------Type de donnees----------------------------------------------
")
sapply(dataset,class)
print("
--------------------------------------------------------------------------
")
print(unique(dataset$gender))
print(unique(dataset$smoking_status))
ggplot(dataset) +
geom_histogram(aes(x = avg_glucose_level))
data_prep <- dataset
# Convertir la variable char en num
data_prep$bmi <- as.numeric(data_prep$bmi)
data_prep
data_prep <- data_prep%>%
mutate(smoking_status = if_else(tolower(smoking_status) == "formerly smoked","smokes",
if_else(tolower(smoking_status) == "unknown",NA_character_,
smoking_status)))
data_prep <- dataset
# Convertir la variable char en num
data_prep$bmi <- as.numeric(data_prep$bmi)
data_prep
data_prep <- data_prep%>%
mutate(smoking_status = if_else(tolower(smoking_status) == "formerly smoked",
"smokes",
if_else(tolower(smoking_status) == "unknown",
NA_character_,
smoking_status)))
data_prep <- dataset
# Convertir la variable char en num
data_prep$bmi <- as.numeric(data_prep$bmi)
data_prep
data_prep <- data_prep%>%
mutate(smoking_status = if_else(tolower(smoking_status) == "formerly smoked",
"smokes",
if_else(tolower(smoking_status) == "unknown",
NA_character_,
smoking_status)))
data_prep <- data_prep%>%
mutate(smoking_status = if_else(tolower(smoking_status) == "formerly smoked",
"smokes",
if_else(tolower(smoking_status) == "unknown",
NA_character_,
smoking_status)))
data_prep
data_prep <- dataset
# Convertir la variable char en num
data_prep$bmi <- as.numeric(data_prep$bmi)
data_prep <- data_prep%>%
mutate(smoking_status = if_else(tolower(smoking_status) == "formerly smoked",
"smokes",
if_else(tolower(smoking_status) == "unknown",
NA_character_,
smoking_status)))
data_prep
data_prep1 <- subset(data_prep, gender != "Other")
data_prep1
print(unique(data_prep1$gender))
print(unique(data_prep1$smoking_status))
# Résumé des valeurs manquantes
md.pattern(data_prep1)
data_prep1
data_prep1$bmi <- ifelse(is.na(data_prep1$bmi),
mean(data_prep1$bmi, na.rm = TRUE),
data_prep1$bmi)
data_prep1$bmi <- ifelse(is.na(data_prep1$bmi),
mean(data_prep1$bmi, na.rm = TRUE),
data_prep1$bmi)
data_prep1 <- data_prep1 %>%
mutate(bmi = round(bmi, 1))
#stroke_recipe <- recipes::recipe(stroke ~. , data = data_prep1)%>%
#step_dummy(all_nominal(), -all_outcomes()) %>%
#step_normalize(all_numeric())
#step_impute_knn(all_predictors())
data_prep2<- na.omit(data_prep1)
print(data_prep2)
#stroke_recipe
ggplot(data_prep2)+
geom_histogram(aes(x=bmi))
data_numeric <- data_prep2[sapply(data_prep2, is.numeric)]
# Calculer la matrice de corrélation
cor_matrix <- cor(data_numeric, use = "complete.obs")
corrplot(cor_matrix, method = "color", type = "upper", tl.col = "black", tl.srt = 45)
# summarize the class distribution
percentage <- prop.table(table(data_prep2$stroke)) * 100
cbind(freq=table(data_prep2$stroke), percentage=percentage)
# Conversion des variables catégorielles en facteurs
data_prep2[] <- lapply(data_prep2, function(x) if(is.character(x)) as.factor(x) else x)
formula <- as.formula(paste("~ ."))
# Utiliser dummyVars pour créer un modèle d'encodage
dummy_model <- dummyVars(formula, data = data_prep2)
# Appliquer le modèle d'encodage pour transformer les données
data_encoded <- predict(dummy_model, newdata = data_prep2)
# Convertir le résultat en data frame
data_encoded <- as.data.frame(data_encoded)
# Ajouter la variable cible au data frame encodé
data_encoded$stroke <- data_prep2$stroke
data_encoded
# summarize the class distribution
percentage <- prop.table(table(data_encoded$stroke)) * 100
# summarize the class distribution
percentage <- prop.table(table(data_encoded$stroke)) * 100
cbind(freq=table(data_encoded$stroke), percentage=percentage)
# rééquilibrer les données
data_smote <- SMOTE(data_encoded[ , -ncol(data_encoded)], data_encoded$stroke, K = 5, dup_size = 10)
str(data_smote)
# Combinaison des données SMOTE générées avec les classes
data_balanced <- data_smote$data
data_balanced$class <- as.factor(data_smote$data$class)
# Vérification de la distribution des classes après rééquilibrage
table(data_balanced$class)
print(head(data_balanced))
# summarize the class distribution
percentage <- prop.table(table(data_balanced$class)) * 100
cbind(freq=table(data_balanced$class), percentage=percentage)
# rééquilibrer les données
data_smote <- SMOTE(data_encoded[ , -ncol(data_encoded)], data_encoded$stroke, K = 5, dup_size = 12)
str(data_smote)
# Combinaison des données SMOTE générées avec les classes
data_balanced <- data_smote$data
data_balanced$class <- as.factor(data_smote$data$class)
# Vérification de la distribution des classes après rééquilibrage
table(data_balanced$class)
print(head(data_balanced))
# summarize the class distribution
percentage <- prop.table(table(data_balanced$class)) * 100
cbind(freq=table(data_balanced$class), percentage=percentage)
data_split <-initial_split(data_balanced,
prop = 3/4)
data_split
data_train <- training(data_split)
data_test <- testing(data_split)
data_cv <- vfold_cv(data_train)
lr_model <-
logistic_reg() %>%
set_args(penalty = tune(), mixture = tune()) %>%
set_engine("glm") %>%
set_mode("classification")
nb_model <-
naive_Bayes()%>%
set_args(smoothness = tune(), Laplace = tune())%>%
set_engine("naivebayes")%>%
set_mode("classification")
svm_model <-
svm_linear() %>%
set_args(cost= tune())%>%
set_engine("kernlab") %>%
set_mode("classification")
#SVM avec noyau radial
svm_model_rbf <-
svm_rbf()%>%
set_args(cost = tune(), rbf_sigma = tune())%>%
set_engine("kernlab") %>%
set_mode("classification")
rf_model <-
rand_forest() %>%
set_args(mtry = tune()) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
lr_model <-
logistic_reg() %>%
set_args(penalty = tune(), mixture = tune()) %>%
set_engine("glm") %>%
set_mode("classification")
nb_model <-
naive_Bayes()%>%
set_args(smoothness = tune(), Laplace = tune())%>%
set_engine("naivebayes")%>%
set_mode("classification")
svm_model <-
svm_linear() %>%
set_args(cost= tune())%>%
set_engine("kernlab") %>%
set_mode("classification")
#SVM avec noyau radial
svm_model_rbf <-
svm_rbf()%>%
set_args(cost = tune(), rbf_sigma = tune())%>%
set_engine("kernlab") %>%
set_mode("classification")
rf_workflow <- workflow()%>%
add_model(rf_model)%>%
add_formula(class ~ .)
#add_recipe(stroke_recipe)
lr_workflow <- workflow()%>%
add_model(lr_model)%>%
add_formula(class ~ .)
#add_recipe(stroke_recipe)
nb_workflow <- workflow()%>%
add_model(nb_model)%>%
add_formula(class ~ .)
install.packages(discrim)
library(discrim)
nb_workflow <- workflow()%>%
add_model(nb_model)%>%
add_formula(class ~ .)
#add_recipe(stroke_recipe)
svm_workflow <- workflow()%>%
add_model(svm_model)%>%
add_formula(class ~ .)
#add_recipe(stroke_recipe)
svm_rbf_workflow <- workflow()%>%
add_model(svm_model_rbf)%>%
add_formula(class ~ .)
#add_recipe(stroke_recipe)
params <- parameters(lr_model)
params2 <- parameters(svm_model)
params3 <-parameters(svm_model_rbf)
rf_grid <- grid_random(mtry(range = c(1,10)),size = 10)
lr_grid <- grid_random(params,size = 10)
rf_grid <- grid_random(mtry(range = c(1,10)),size = 10)
lr_grid <- grid_random(params,size = 10)
rf_grid <- grid_random(mtry(range = c(1,10)),size = 10)
lr_grid <- grid_random(parameters(lr_model),size = 10)
rf_model <-
rand_forest() %>%
set_args(mtry = tune()) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
lr_model <- logistic_reg() %>%
set_args(penalty = tune(), mixture = tune()) %>%
set_engine("glm") %>%
set_mode("classification")
nb_model <-
naive_Bayes()%>%
set_args(smoothness = tune(), Laplace = tune())%>%
set_engine("naivebayes")%>%
set_mode("classification")
svm_model <-
svm_linear() %>%
set_args(cost= tune())%>%
set_engine("kernlab") %>%
set_mode("classification")
#SVM avec noyau radial
svm_model_rbf <-
svm_rbf()%>%
set_args(cost = tune(), rbf_sigma = tune())%>%
set_engine("kernlab") %>%
set_mode("classification")
lr_workflow <- workflow()%>%
add_model(lr_model)%>%
add_formula(class ~ .)
#add_recipe(stroke_recipe)
params <- parameters(lr_model)
params2 <- parameters(svm_model)
params3 <-parameters(svm_model_rbf)
rf_grid <- grid_random(mtry(range = c(1,10)),size = 10)
lr_grid <- grid_random(parameters(lr_model),size = 10)
rf_grid <- grid_random(mtry(range = c(1,10)),size = 10)
lr_grid <- grid_random(params,size = 10)
params <- parameters(lr_model)
params2 <- parameters(svm_model)
params3 <-parameters(svm_model_rbf)
rf_grid <- grid_random(mtry(range = c(1,10)),size = 10)
lr_grid <- grid_random(params,size = 10)
rf_grid <- grid_random(mtry(range = c(1,10)),size = 10)
lr_grid <- grid_random(penalty = c(0.001, 0.01, 0.1, 1, 10),mixture = c(0, 0.5, 1),size = 10)
rf_grid <- grid_random(mtry(range = c(1,10)),size = 10)
lr_grid <- grid_random(params,size = 10)
params <- parameters(lr_model)
params2 <- parameters(svm_model)
params3 <-parameters(svm_model_rbf)
rf_grid <- grid_random(mtry(range = c(1,10)),size = 10)
lr_grid <- grid_random(params,size = 10)
rf_grid <- grid_random(mtry(range = c(1,10)),size = 10)
lr_grid <- grid_random(parameters(lr_model),size = 10)
nb_grid <- grid_random(smoothness(range = c(0, 1)),Laplace(range = c(0, 1)), size = 10)
svm_grid <- grid_random(params2, size = 10)
svm_rbf_grid <- grid_random(params3, size = 10)
lr_grid <- grid_random(parameters(lr_model),size = 10)
lr_model <- logistic_reg() %>%
set_args(penalty = tune(), mixture = tune()) %>%
set_engine("glm") %>%
lr_model <- logistic_reg() %>%
set_args(penalty = tune(), mixture = tune()) %>%
set_engine("glm") %>%
set_mode("classification")
lr_model <- logistic_reg() %>%
set_args(penalty = tune()) %>%
set_engine("glm") %>%
set_mode("classification")
lr_workflow <- workflow()%>%
add_model(lr_model)%>%
add_formula(class ~ .)
#add_recipe(stroke_recipe)
params <- parameters(lr_model)
params2 <- parameters(svm_model)
params3 <-parameters(svm_model_rbf)
lr_grid <- grid_random(parameters(lr_model),size = 10)
lr_grid <- grid_random(params,size = 10)
lr_grid <- grid_random(params,size = 10)
#lr_grid <- grid_random(params,size = 10)
set.seed(123)
data_folds <-vfold_cv(data_balanced, v = 5)
print(data_folds$splits[[1]])
dt_results <-dt_workflow %>%
tune_grid(resamples = data_folds, grid = dt_grid)
tune_grid(resamples = data_folds, grid = dt_grid)
